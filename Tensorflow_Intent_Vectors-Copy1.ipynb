{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query_1</th>\n",
       "      <th>Query_2</th>\n",
       "      <th>Common_Words</th>\n",
       "      <th>Uncommon_Words_Query_1</th>\n",
       "      <th>Uncommon_Words_Query_2</th>\n",
       "      <th>Intent_1</th>\n",
       "      <th>Intent_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>who will a approve my &lt;t&gt;</td>\n",
       "      <td>who to connect with for &lt;t&gt; balance upgradation</td>\n",
       "      <td>[who]</td>\n",
       "      <td>[approve]</td>\n",
       "      <td>[balance]</td>\n",
       "      <td>who will approve my leave</td>\n",
       "      <td>who will approve my leave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>who will a approve my &lt;t&gt;</td>\n",
       "      <td>who is the poc for &lt;t&gt; related issues</td>\n",
       "      <td>[who]</td>\n",
       "      <td>[approve]</td>\n",
       "      <td>[issue]</td>\n",
       "      <td>who will approve my leave</td>\n",
       "      <td>who will approve my leave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>who will a approve my &lt;t&gt;</td>\n",
       "      <td>i have to delete my time booking which is appr...</td>\n",
       "      <td>[approve]</td>\n",
       "      <td>[who]</td>\n",
       "      <td>[time, which, want, apply]</td>\n",
       "      <td>who will approve my leave</td>\n",
       "      <td>who will approve my leave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>who will a approve my &lt;t&gt;</td>\n",
       "      <td>is there anyone who should be contacted if the...</td>\n",
       "      <td>[who]</td>\n",
       "      <td>[approve]</td>\n",
       "      <td>[anyone, contact, mismatch]</td>\n",
       "      <td>who will approve my leave</td>\n",
       "      <td>who will approve my leave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>who will a approve my &lt;t&gt;</td>\n",
       "      <td>do you know who is the &lt;t&gt; &lt;t&gt; to update &lt;t&gt; b...</td>\n",
       "      <td>[who]</td>\n",
       "      <td>[approve]</td>\n",
       "      <td>[update, balance]</td>\n",
       "      <td>who will approve my leave</td>\n",
       "      <td>who will approve my leave</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Query_1  \\\n",
       "0  who will a approve my <t>    \n",
       "1  who will a approve my <t>    \n",
       "2  who will a approve my <t>    \n",
       "3  who will a approve my <t>    \n",
       "4  who will a approve my <t>    \n",
       "\n",
       "                                             Query_2 Common_Words  \\\n",
       "0    who to connect with for <t> balance upgradation        [who]   \n",
       "1              who is the poc for <t> related issues        [who]   \n",
       "2  i have to delete my time booking which is appr...    [approve]   \n",
       "3  is there anyone who should be contacted if the...        [who]   \n",
       "4  do you know who is the <t> <t> to update <t> b...        [who]   \n",
       "\n",
       "  Uncommon_Words_Query_1       Uncommon_Words_Query_2  \\\n",
       "0              [approve]                    [balance]   \n",
       "1              [approve]                      [issue]   \n",
       "2                  [who]   [time, which, want, apply]   \n",
       "3              [approve]  [anyone, contact, mismatch]   \n",
       "4              [approve]            [update, balance]   \n",
       "\n",
       "                    Intent_1                   Intent_2  \n",
       "0  who will approve my leave  who will approve my leave  \n",
       "1  who will approve my leave  who will approve my leave  \n",
       "2  who will approve my leave  who will approve my leave  \n",
       "3  who will approve my leave  who will approve my leave  \n",
       "4  who will approve my leave  who will approve my leave  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('test_pairs.pkl')\n",
    "\n",
    "mask = []\n",
    "\n",
    "#removing empty entries from uncommon2\n",
    "\n",
    "for index,words in enumerate(df.Uncommon_Words_Query_2):\n",
    "    \n",
    "    if len(words) == 0:\n",
    "        mask.append(index)\n",
    "\n",
    "df = df.drop(mask)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('query_pairs_fixed.pkl')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data into lists\n",
    "\n",
    "n = df.shape[0]\n",
    "\n",
    "#n = 1000 #partial data for checking implementation\n",
    "\n",
    "intent_1 = list(df.Intent_1.iloc[:n])    \n",
    "intent_2 = list(df.Intent_2.iloc[:n])\n",
    "\n",
    "uncommon1 = list(df.Uncommon_Words_Query_1.iloc[:n])\n",
    "uncommon2 = list(df.Uncommon_Words_Query_2.iloc[:n])\n",
    "\n",
    "common = list(df.Common_Words.iloc[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique intents = 5\n"
     ]
    }
   ],
   "source": [
    "#Create a dictionary of intent types to initialize embeddings\n",
    "\n",
    "unique_intents = list(set(intent_1 + intent_2))\n",
    "print('Total unique intents = {}'.format(len(unique_intents)))\n",
    "intent_index = list(np.arange(0, len(unique_intents)))\n",
    "\n",
    "intent_dict = dict(zip(unique_intents, intent_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of vocabulary (other permanent dictionary stored in the Dictionaries.py file)\n",
    "\n",
    "def get_vocab_dict(lol):\n",
    "    \n",
    "    #lol = list of lists (of strings)\n",
    "\n",
    "    vocab = []\n",
    "    for words in lol:\n",
    "        \n",
    "        for word in words:\n",
    "            \n",
    "            vocab.append(word)\n",
    "            \n",
    "    vocab = list(set(vocab))\n",
    "    values = list(np.arange(0, len(vocab)))\n",
    "    \n",
    "    vocab_dict = dict(zip(vocab, values))\n",
    "            \n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create vocabulary dictionary of all words \n",
    "\n",
    "vocab_dict = get_vocab_dict(uncommon1 + uncommon2)\n",
    "len(vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert words in uncommon1 and uncommon2 to their respective indices in vocab_dict\n",
    "\n",
    "unc1_index = []\n",
    "unc2_index = []\n",
    "\n",
    "for words  in uncommon1:\n",
    "\n",
    "    indices1 = []\n",
    "    for word in words:\n",
    "        \n",
    "        dict_value = vocab_dict[word]\n",
    "        indices1.append(dict_value)\n",
    "        \n",
    "    unc1_index.append(indices1)\n",
    "\n",
    "    \n",
    "for words in uncommon2:\n",
    "    \n",
    "    indices2 = []\n",
    "    for word in words:\n",
    "        \n",
    "        dict_value = vocab_dict[word]\n",
    "        indices2.append(dict_value)\n",
    "        \n",
    "    unc2_index.append(indices2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert named intents to their corresponding value in the dictionary\n",
    "\n",
    "intent1_index = []\n",
    "intent2_index = []\n",
    "\n",
    "for intent in intent_1:\n",
    "     \n",
    "    l = []\n",
    "    intent_value = intent_dict[intent]\n",
    "    l.append(intent_value)\n",
    "    intent1_index.append(l)\n",
    "    \n",
    "for intent in intent_2:\n",
    "    \n",
    "    l = []\n",
    "    intent_value = intent_dict[intent]\n",
    "    l.append(intent_value)\n",
    "    intent2_index.append(l)\n",
    "    \n",
    "    #Get intent indices for negative samples\n",
    "    \n",
    "intent1_index = np.asarray(intent1_index)\n",
    "intent2_index = np.asarray(intent2_index)\n",
    "\n",
    "negative_index = []\n",
    "\n",
    "for i in range(len(intent2_index)):\n",
    "    \n",
    "    l = []\n",
    "    negative_intent = np.random.randint(0, len(unique_intents) - 1)\n",
    "    if negative_intent >= intent2_index[i]:\n",
    "        \n",
    "        negative_intent += 1\n",
    "        \n",
    "    l.append(negative_intent)\n",
    "    negative_index.append(l)\n",
    "\n",
    "negative_index = np.asarray(negative_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\visha\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#initialize embeddings for intents and words (the current initialization used is from OpenKE's KE model)\n",
    "\n",
    "embed_dim = 8\n",
    "\n",
    "intent_embeddings = tf.get_variable(name = 'intent_embeddings', shape = [len(unique_intents), embed_dim], initializer = \n",
    "                                    tf.contrib.layers.xavier_initializer(uniform = False))\n",
    "\n",
    "word_embeddings = tf.get_variable(name = 'word_embeddings', shape = [len(vocab_dict), embed_dim], initializer = \n",
    "                                    tf.contrib.layers.xavier_initializer(uniform = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\visha\\Anaconda3\\envs\\Tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "#SINGLE INSTANCE BATCH\n",
    "#--------------------------------------------------------------------------------\n",
    "u1_batch = tf.placeholder(tf.int32, shape=[None, ])\n",
    "u2_batch = tf.placeholder(tf.int32, shape=[None, ])\n",
    "\n",
    "i1_batch = tf.placeholder(tf.int32, shape=[None, ])\n",
    "i2_batch = tf.placeholder(tf.int32, shape=[None, ])\n",
    "n_batch  = tf.placeholder(tf.int32, shape=[None, ])\n",
    "\n",
    "#generating training examples using embedding_lookup\n",
    "i1 = tf.nn.l2_normalize(tf.nn.embedding_lookup(intent_embeddings, i1_batch), axis=1)\n",
    "i2 = tf.nn.l2_normalize(tf.nn.embedding_lookup(intent_embeddings, i2_batch), axis=1)\n",
    "i3 = tf.nn.l2_normalize(tf.nn.embedding_lookup(intent_embeddings, n_batch), axis=1)\n",
    " \n",
    "unc1_vector = tf.reduce_sum(tf.nn.l2_normalize(tf.nn.embedding_lookup(word_embeddings, u1_batch),axis=1), 0)\n",
    "unc2_vector = tf.reduce_sum(tf.nn.l2_normalize(tf.nn.embedding_lookup(word_embeddings, u1_batch),axis=1), 0)\n",
    "\n",
    "epsilon = 1e-15\n",
    "\n",
    "#loss = tf.nn.l2_loss(i1 - i2)\n",
    "#loss = tf.norm((i1 - i2) - (unc1_vector - unc2_vector)) - tf.norm((i1 - i3) - (unc1_vector - unc2_vector))\n",
    "loss = tf.norm((i1 - i2 + epsilon) - (unc1_vector - unc2_vector)) - tf.norm((i1 - i3 + epsilon) - (unc1_vector - unc2_vector)) \n",
    "#loss = tf.norm((i1 - unc1_vector)) + tf.norm((i2 - unc2_vector))\n",
    "\n",
    "\n",
    "#tf.trainable_variables()\n",
    "optimizer  = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 79.60it/s]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    init_loss = 0\n",
    "    final_loss = 0\n",
    "    \n",
    "#    for epochs in (range(7)):\n",
    "    \n",
    "\n",
    "    for i in tqdm(range(100)):\n",
    "\n",
    "\n",
    "        feed_dict = {i1_batch : intent1_index[i], i2_batch : intent2_index[i], n_batch : negative_index[i],\n",
    "                     u1_batch : unc1_index[i], u2_batch : unc2_index[i]}\n",
    "\n",
    "\n",
    "        init_loss += sess.run(loss, feed_dict)\n",
    "        sess.run(train_op, feed_dict)\n",
    "        final_loss += sess.run(loss, feed_dict)\n",
    "\n",
    "\n",
    "    intent_vectors = sess.run(intent_embeddings)\n",
    "    word_vectors = sess.run(word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-124.95627278089523, -125.80549603700638)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_loss, final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save obtained vectors as numpy arrays to be used in future\n",
    "\n",
    "# np.save('word_vectors', word_vectors)\n",
    "# np.save('intent_vectors', intent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert obtained vectors to TSV files for visualization using embedding projector (http://projector.tensorflow.org/)\n",
    "\n",
    "np.savetxt('w_vec.txt', word_vectors , delimiter='\\t')\n",
    "np.savetxt('i_vec.txt', intent_vectors , delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving word and intent keys for obtained vectors to be used as metadata for the visualizer\n",
    "\n",
    "import csv   \n",
    "\n",
    "wkeys = []\n",
    "\n",
    "for key in vocab_dict.keys():\n",
    "    wkeys.append(key)\n",
    "    \n",
    "      \n",
    "with open('w_keys.txt', 'w') as f_output:\n",
    "    tsv_output = csv.writer(f_output, delimiter='\\n')\n",
    "    tsv_output.writerow(wkeys)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ikeys = []\n",
    "\n",
    "for key in intent_dict.keys():\n",
    "    ikeys.append(key)\n",
    "    \n",
    "\n",
    "with open('i_keys.txt', 'w') as f_output:\n",
    "    tsv_output = csv.writer(f_output, delimiter='\\n')\n",
    "    tsv_output.writerow(ikeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending both intent and word vectors for combined visualization \n",
    "\n",
    "main = np.concatenate((word_vectors, intent_vectors), axis=0)\n",
    "np.savetxt('main.txt', main , delimiter='\\t')\n",
    "\n",
    "#metadata for combined visualization\n",
    "\n",
    "main_keys = []\n",
    "\n",
    "for keys in vocab_dict.keys():  \n",
    "    main_keys.append(keys)\n",
    "    \n",
    "for keys in intent_dict.keys(): \n",
    "    main_keys.append(keys)\n",
    "    \n",
    "    \n",
    "with open('main_keys.txt', 'w') as f_output:\n",
    "    tsv_output = csv.writer(f_output, delimiter='\\n')\n",
    "    tsv_output.writerow(main_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------FOR EVALUTATION OF VECTORS-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v(word):\n",
    "    \n",
    "    word_index = vocab_dict[word]\n",
    "    word_vector = word_vectors[word_index]\n",
    "    \n",
    "    return word_vector\n",
    "\n",
    "def intent2vec(intent):\n",
    "    \n",
    "    intent_index = intent_dict[intent]\n",
    "    intent_vector = intent_vectors[intent_index]\n",
    "    \n",
    "    return intent_vector\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict['how']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approval = w2v('approval')     #w2v function finds the embedding of the word in the trained embeddings\n",
    "left = w2v('left')\n",
    "remain = w2v('remain')\n",
    "step = w2v('step')\n",
    "apply = w2v('apply')\n",
    "take = w2v('take')\n",
    "procedure = w2v('procedure')\n",
    "how = w2v('how')\n",
    "many = w2v('many')\n",
    "encash = w2v('encash')\n",
    "balance = w2v('balance')\n",
    "encashment = w2v('encashment')\n",
    "year = w2v('year')\n",
    "beginning = w2v('beginning')\n",
    "provide = w2v('provide')\n",
    "new = w2v('new')\n",
    "employee = w2v('employee')\n",
    "month = w2v('month')\n",
    "every = w2v('every')\n",
    "adopt = w2v('adopt')\n",
    "birth = w2v('birth')\n",
    "second = w2v('second')\n",
    "third = w2v('third')\n",
    "application = w2v('application')\n",
    "process = w2v('process')\n",
    "\n",
    "\n",
    "\n",
    "many_kids = intent2vec('will i get maternity leave for more than two children')    #intent2vec finds embedding of intent in trained embeds\n",
    "intent_vec2 = intent2vec('how to apply leave')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remain , left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,1]\n",
    "b = [2,2]\n",
    "cosine(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adopt + birth + second + third , many_kids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many + provide + every + month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginning + new + year + many + provide + employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "(approval + left) , intent_vectors[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "dists = []\n",
    "\n",
    "e1 = intent_vectors[18]\n",
    "\n",
    "for index, intent in enumerate(intent_vectors):\n",
    "    \n",
    "    if index == 18:\n",
    "        \n",
    "        continue\n",
    "        \n",
    "    dists.append(cosine(e1, intent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, val in enumerate(dists):\n",
    "    \n",
    "    if val == min(dists):\n",
    "\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('Intent_Embeddings', intent_vectors, allow_pickle=True)\n",
    "#np.save('Word_Embeddings', word_vectors, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def bow(text):\n",
    "    \n",
    "    all_words = []\n",
    "    \n",
    "    for sentence in text:\n",
    "        \n",
    "        for word in sentence:\n",
    "            \n",
    "            all_words.append(word.lower())\n",
    "            \n",
    "    all_words = FreqDist(all_words)\n",
    "\n",
    "    return all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words8 = np.load('Word_Embeddings_8.npy', allow_pickle=True)\n",
    "intents8 = np.load('Intent_Embeddings_8.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words30 = np.load('Word_Embeddings.npy', allow_pickle=True)\n",
    "intents30 = np.load('Intent_Embeddings.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualify = w2v('qualify')\n",
    "eligible = w2v('eligible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(qualify - apply, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------POSSIBLE APPROACHED TO CREATE BATCHES (Work in Progress)--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#using padding to make all arrays of same lenght and then retrieving the required indices \n",
    "#(possible method to create batches for un1, unc2), needs further investigation\n",
    "\n",
    "a = tf.convert_to_tensor(np.array([0,1,2,3,0]))\n",
    "b = tf.placeholder(tf.int32, shape=[None, 5])\n",
    "\n",
    "data_mask = tf.cast(a, tf.bool)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "sess.run(init)\n",
    "print(sess.run(b, {b:sess.run(a)}))\n",
    "for i in b:\n",
    "    \n",
    "    print(sess.run(i))\n",
    "print(tf.shape(data_mask))\n",
    "print(sess.run(data_mask))\n",
    "print(sess.run(vals))\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_batch(batch_size):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "start = tf.placeholder(tf.int32, shape=(1,))\n",
    "end = tf.placeholder(tf.int32, shape=(1,))\n",
    "\n",
    "current_pos = 0\n",
    "batch_size = 5\n",
    "\n",
    "def get_batch(current_pos, batch_size):\n",
    "    \n",
    "    start_pos = current_pos\n",
    "    end_pos = current_pos + batch_size\n",
    "    \n",
    "    last = end_pos\n",
    "\n",
    "    return start_pos, end_pos, last\n",
    "\n",
    "asd = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    \n",
    "    s.run(asd)\n",
    "    start, end, last = get_batch(current_pos, batch_size)\n",
    "    intent1_batch = intent1_index[:end]\n",
    "    print(intent1_batch)\n",
    "#i1 = tf.nn.embedding_lookup(intent_embeddings, intent1_index)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "#finding lenght of each training instance\n",
    "\n",
    "\n",
    "unc1_len = []\n",
    "\n",
    "for item in unc1_index:\n",
    "    unc1_len.append(len(item))\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "\n",
    "unc2_len = []\n",
    "\n",
    "for item in unc2_index:\n",
    "    unc2_len.append(len(item))\n",
    "    \n",
    "unc2_len[:3], unc1_len[:3]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "padded_size = 9\n",
    "\n",
    "all_zero = np.zeros((padded_size)).astype(int)\n",
    "unc1_padded = []\n",
    "for item in unc1_index:\n",
    "    \n",
    "    copy = all_zero.copy()\n",
    "    copy[:len(item)] = item\n",
    "    unc1_padded.append(copy)\n",
    "    \n",
    "unc1_padded = np.asarray(unc1_padded)\n",
    "unc1_padded[:6]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "padded_size = 9\n",
    "\n",
    "all_zero = np.zeros((padded_size)).astype(int)\n",
    "unc2_padded = []\n",
    "for item in unc2_index:\n",
    "    \n",
    "    copy = all_zero.copy()\n",
    "    copy[:len(item)] = item\n",
    "    unc2_padded.append(copy)\n",
    "    \n",
    "unc2_padded = np.asarray(unc2_padded)\n",
    "unc2_padded[:6]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_batch(start, batch_size):\n",
    "    \n",
    "    initial = start\n",
    "    final = start + batch_size\n",
    "    \n",
    "    current_pos = final\n",
    "    \n",
    "    return initial, final, current_pos\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "current_pos = 0\n",
    "\n",
    "num_batches = n // batch_size\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "u1 = tf.placeholder(tf.int32, shape=[None, 9])\n",
    "u2 = tf.placeholder(tf.int32, shape=[None, 9])\n",
    "\n",
    "i1_batch = tf.placeholder(tf.int32, shape=[None, ])\n",
    "i2_batch = tf.placeholder(tf.int32, shape=[None, ])\n",
    "n_batch = tf.placeholder(tf.int32, shape=[None, ])\n",
    "\n",
    "i1 = tf.nn.embedding_lookup(intent_embeddings, i1_batch)\n",
    "i2 = tf.nn.embedding_lookup(intent_embeddings, i2_batch)\n",
    "negative = tf.nn.embedding_lookup(intent_embeddings, n_batch)\n",
    "\n",
    "\n",
    "\n",
    "loss = tf.reduce_sum(tf.norm((i1-i2) - (unc1 - unc2)) - tf.norm((i1-negative) - (unc1 - unc2)))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as s:\n",
    "    \n",
    "    s.run(init)\n",
    "    \n",
    "    for j in range(5):\n",
    "        \n",
    "        start, end, current_pos = get_batch(current_pos, batch_size)\n",
    "        print(start, end)\n",
    "        \n",
    "#-----------------------------------------------------------------------------------\n",
    "\n",
    "        mask1 = s.run(u1, {u1: unc1_padded[start:end]})\n",
    "        unc1 = []\n",
    "        for index, val in enumerate(unc1_len[start:end]):\n",
    "            \n",
    "            e_id = mask1[index][:val]\n",
    "            emb = tf.nn.embedding_lookup(word_embeddings, e_id)\n",
    "            one_sum = tf.reduce_sum(emb, 0)\n",
    "            unc1.append(one_sum)\n",
    "            \n",
    "        unc1 = tf.convert_to_tensor(unc1)\n",
    "        \n",
    "#-----------------------------------------------------------------------------------\n",
    "        \n",
    "        mask2 = s.run(u2, {u2: unc2_padded[start:end]})\n",
    "\n",
    "        unc2 = []\n",
    "        for index, val in enumerate(unc2_len[start:end]):\n",
    "            \n",
    "            e_id = mask2[index][:val]\n",
    "            emb = tf.nn.embedding_lookup(word_embeddings, e_id)\n",
    "            one_sum = tf.reduce_sum(emb, 0)\n",
    "            unc2.append(one_sum)\n",
    "            \n",
    "        unc2 = tf.convert_to_tensor(unc2)\n",
    "    \n",
    "#------------------------------------------------------------------------------------\n",
    "        feed_dict={i1_batch : intent1_index[start:end], i2_batch : intent2_index[start:end],\n",
    "                   n_batch : negative_index[start:end]}\n",
    "    \n",
    "#---------------------------------------------------------------------------------------\n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "        s.run(optimizer, feed_dict)\n",
    "        \n",
    "    intent_vectors = s.run(intent_embeddings)\n",
    "    word_vectors = s.run(word_embeddings)\n",
    "        \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------ACTUAL CODE ENDS HERE, ONY TESTS BEYOND THIS POINT----------------------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "#this works and can be used to create batches for i1, i2, i3 but not for unc1, unc2\n",
    "\n",
    "i1_index = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "i1 = tf.nn.embedding_lookup(intent_embeddings, i1_index)\n",
    "\n",
    "loss = tf.norm(i1)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "optimizer  = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "def get_batch(start, batch_size):\n",
    "    \n",
    "    initial = start\n",
    "    final = start + batch_size\n",
    "    \n",
    "    current_pos = final\n",
    "    \n",
    "    return initial, final, current_pos\n",
    "\n",
    "batch_size = 5\n",
    "current_pos = 0\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    start, end, current_pos = get_batch(current_pos, batch_size)\n",
    "    print(start, end)\n",
    "    \n",
    "    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    batch_size = 5\n",
    "    current_pos = 0\n",
    "\n",
    "    print(sess.run(i1, feed_dict={i1_index: intent2_index[:10]}))\n",
    "    print('---------------------')\n",
    "    \n",
    "    for j in range(2):\n",
    "        \n",
    "        start, end, current_pos = get_batch(current_pos, batch_size)\n",
    "        print(sess.run(i1, feed_dict={i1_index: intent2_index[start:end]}))\n",
    "        print(sess.run(loss, feed_dict={i1_index: intent2_index[start:end]}))\n",
    "        \n",
    "        for i in range(100):\n",
    "            sess.run(train_op, feed_dict={i1_index: intent2_index[start:end]})\n",
    "            \n",
    "        print(sess.run(loss, feed_dict={i1_index: intent2_index[start:end]}))\n",
    "        print(sess.run(i1, feed_dict={i1_index: intent2_index[start:end]}))\n",
    "        \n",
    "    print('-------------------------')  \n",
    "    print(sess.run(i1, feed_dict={i1_index: intent2_index[:10]}))\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "#trying methods to create batches for unc1, unc2\n",
    "\n",
    "i1_index = tf.placeholder(tf.int32, shape=[None])\n",
    "i1 = tf.nn.embedding_lookup(intent_embeddings, i1_index)\n",
    "\n",
    "u2_index = tf.placeholder(tf.int32, shape=(5,))\n",
    "u2 = tf.unstack(tf.reshape(u2_index, [-1])) #tf cannot iterate over tensor object so using stack to get around that for now\n",
    "\n",
    "\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    print('initial embedding ', sess.run(word_embeddings[3]))\n",
    "    \n",
    "    \n",
    "    batch_u2 = unc2_len[:batch_size]\n",
    "    unc2_vector = []\n",
    "    count = 0\n",
    "    for size in u2:\n",
    "        \n",
    "        l = int(sess.run(size, {u2_index: batch_u2}))   #choosing some instances to train\n",
    "        end = count + l  \n",
    "\n",
    "        word_index = unc2_concat[:l]   #somehow 'l' works but replacing 'l' with 'end' does not work, need to look at error\n",
    "        w_vec = tf.nn.embedding_lookup(word_embeddings, word_index)\n",
    "        one_sum = tf.reduce_sum(w_vec, 0)\n",
    "        unc2_vector.append(one_sum)\n",
    "        count += size\n",
    "        \n",
    "    print(sess.run(unc2_vector))\n",
    "        \n",
    "    loss = tf.norm(i1 - unc2_vector)\n",
    "    optimizer  = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "    \n",
    "    print(sess.run(loss))\n",
    "    for i in range(1000):\n",
    "        \n",
    "        sess.run(train_op, {i1_index : intent1_index[:5]})\n",
    "    \n",
    "    print(sess.run(loss))\n",
    "    print('trained embedding ',sess.run(word_embeddings[0]))\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "#this sort of works but need to figure out how to implement in sync with rest of the code\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#generating training examples using embedding_lookup\n",
    "\n",
    "\n",
    "i1 = tf.nn.embedding_lookup(intent_embeddings, intent1_index)\n",
    "i2 = tf.nn.embedding_lookup(intent_embeddings, intent2_index)\n",
    "i3 = tf.nn.embedding_lookup(intent_embeddings, negative_index)\n",
    "   \n",
    "#adding embeddings of individual words for a training example to form Σuc1, Σuc2\n",
    " \n",
    "unc1_vector = []\n",
    "for mask in unc1_index:\n",
    "       \n",
    "    w_vec = (tf.nn.embedding_lookup(word_embeddings, mask))\n",
    "    one1_sum = tf.reduce_sum(w_vec, 0)\n",
    "    unc1_vector.append(one1_sum)\n",
    "       \n",
    "unc1_vector = tf.convert_to_tensor(unc1_vector)\n",
    "\n",
    "        \n",
    "unc2_vector = []\n",
    "for mask in unc2_index:\n",
    "        \n",
    "    u_vec = (tf.nn.embedding_lookup(word_embeddings, mask))\n",
    "    one2_sum = tf.reduce_sum(u_vec, 0)\n",
    "    unc2_vector.append(one2_sum)\n",
    "        \n",
    "unc2_vector = tf.convert_to_tensor(unc2_vector)\n",
    "\n",
    "\n",
    "#loss = tf.reduce_sum(abs((a-b) - (c-d)) - abs((a-e) - (c-d)))\n",
    "loss = tf.reduce_sum(tf.norm((i1-i2) - (unc1_vector - unc2_vector)) - tf.norm((i1-i3) - (unc1_vector - unc2_vector)))\n",
    "\n",
    "#loss = tf.nn.l2_loss((i1 - unc1_vector))\n",
    "\n",
    "\n",
    "\n",
    "#tf.trainable_variables()\n",
    "optimizer  = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------\n",
    "#SINGLE INSTANCE BATCH\n",
    "#--------------------------------------------------------------------------------\n",
    "u1_batch = tf.placeholder(tf.int32, shape=[None,])\n",
    "u2_batch = tf.placeholder(tf.int32, shape=[None,])\n",
    "\n",
    "i1_batch = tf.placeholder(tf.int32, shape=[None, ])\n",
    "i2_batch = tf.placeholder(tf.int32, shape=[None, ])\n",
    "n_batch = tf.placeholder(tf.int32, shape=[None, ])\n",
    "\n",
    "#generating training examples using embedding_lookup\n",
    "\n",
    "\n",
    "\n",
    "i1 = tf.nn.embedding_lookup(intent_embeddings, i1_batch)\n",
    "i2 = tf.nn.embedding_lookup(intent_embeddings, i2_batch)\n",
    "i3 = tf.nn.embedding_lookup(intent_embeddings, n_batch)\n",
    "   \n",
    "#adding embeddings of individual words for a training example to form Σuc1, Σuc2\n",
    " \n",
    "unc1_vector = tf.reduce_sum((tf.nn.embedding_lookup(word_embeddings, u1_batch)), 0)\n",
    "\n",
    "       \n",
    "unc2_vector = tf.reduce_sum((tf.nn.embedding_lookup(word_embeddings, u2_batch)), 0)\n",
    "\n",
    "\n",
    "\n",
    "#loss = tf.reduce_sum(abs((a-b) - (c-d)) - abs((a-e) - (c-d)))\n",
    "loss = tf.reduce_sum(tf.norm((i1-i2) - (unc1_vector - unc2_vector)) - tf.norm((i1-i3) - (unc1_vector - unc2_vector)))\n",
    "\n",
    "#loss = tf.nn.l2_loss((i1 - unc1_vector))\n",
    "\n",
    "\n",
    "\n",
    "#tf.trainable_variables()\n",
    "optimizer  = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    print(sess.run(word_embeddings[0:5]))\n",
    "\n",
    "    \n",
    "#    print(sess.run(loss, {i1_batch : intent1_index, i2_batch : intent2_index, n_batch : negative_index,\n",
    "#                u1_batch : unc1_index, u2_batch : unc2_index}))\n",
    "\n",
    "    init_loss = 0\n",
    "    final_loss = 0\n",
    "    \n",
    "    for i in range(1000):\n",
    "        \n",
    "        feed_dict = {i1_batch : intent1_index[i], i2_batch : intent2_index[i], n_batch : negative_index[i],\n",
    "                     u1_batch : unc1_index[i], u2_batch : unc2_index[i]}\n",
    "        \n",
    "        init_loss += sess.run(loss, feed_dict)\n",
    "        sess.run(train_op, feed_dict)\n",
    "        final_loss += sess.run(loss, feed_dict)\n",
    "        \n",
    "        if i%2000 == 0:\n",
    "            \n",
    "            print(i)\n",
    "        \n",
    "    print(sess.run(word_embeddings[:5]))\n",
    "\n",
    "\n",
    "    intent_vectors = sess.run(intent_embeddings)\n",
    "    word_vectors = sess.run(word_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word):\n",
    "    \n",
    "    word_index = vocab_dict[word]\n",
    "    word_vector = word_vectors[word_index]\n",
    "    \n",
    "    return word_vector\n",
    "\n",
    "def intent2vec(intent):\n",
    "    \n",
    "    intent_index = intent_dict[intent]\n",
    "    intent_vector = intent_vectors[intent_index]\n",
    "    \n",
    "    return intent_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot((step + apply) , (procedure + take))     #step + apply ~= procedure + take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step + apply - take    #(q1,q2) = (step to apply leave, procedure to take leave) --- (step == procedure, apply == take)\n",
    "                        # which means step + apply - take == procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure     #as can be seen here (used dataset is small and training iterations are more so values are really close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     #intent1 - intent2 == unc1 - unc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    print(sess.run(i1[0]))\n",
    "#    print(sess.run(intent_embeddings[intent1_index[0]]))\n",
    "#    print(sess.run(word_embeddings[unc1_index[0][0]]))\n",
    "    \n",
    "#    print('embedding of intent 1 - ', sess.run(intent_embeddings[intent1_index[0]]))\n",
    "#    print('embedding of unc1 words - ', sess.run(word_embeddings[unc1_index[0][0]]))\n",
    "#    print(sess.run(word_embeddings[unc1_index[0][1]]))\n",
    "#    print('first five word embeddings - ')\n",
    "#    old = sess.run(word_embeddings)\n",
    "#    print(old)\n",
    "#    print('-------------')\n",
    "#    print('initially i1 - unc1 -', sess.run(i1 - unc1_vector)[0])\n",
    "#    for batch_num in range(0, n, batch_size):\n",
    "#---------------------------------------------\n",
    "\n",
    "#        print(sess.run((i1, i2, i3, unc1_vector, unc2_vector), feed_dict ))\n",
    "#        sess.run(train_op, feed_dict)  \n",
    "        \n",
    "#    print(sess.run(loss, {i1_batch : intent1_index, i2_batch : intent2_index, n_batch : negative_index,\n",
    "#                u1_batch : unc1_index, u2_batch : unc2_index}))\n",
    "    \n",
    "#    print('\\n')\n",
    "#    print('embedding of intent 1 - ', sess.run(intent_embeddings[intent1_index[0]]))\n",
    "#    print('embedding of unc1 words - ', sess.run(word_embeddings[unc1_index[0][0]]))\n",
    "#    print(sess.run(word_embeddings[unc1_index[0][1]]))\n",
    "#    print('updated word embeddings')\n",
    "#    new = sess.run(word_embeddings)\n",
    "#    print(old - new)\n",
    "    \n",
    "#    print('---------------')\n",
    "#    print('finally i1 - unc1 ', sess.run(i1 - unc1_vector)[0])\n",
    "\n",
    "#    print(sess.run(i1 - unc1_vector))\n",
    "\n",
    "#    for epoch in range(epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tensorflow]",
   "language": "python",
   "name": "conda-env-Tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
